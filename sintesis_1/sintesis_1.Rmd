---
title: "Síntesis 1"
output: html_notebook
bibliography: /Users/pablohidalgo/Documents/library.bib
---

# Introducción

# 2. Decribir la relación entre autómatas deterministas y no deterministas. El concepto de estrategia de búsqueda.

En la teoría de autómatas, una máquina de estados finitos se considera un autómata finito determinista si:

- cada una de sus transiciones está determinada unívocamente por su estado de entrada y su símbolo de entrada
- 

Estrictamente, un autómata determinista se puede considerar como un tipo de autómata no determinista

En [@Rabin1959] se definen los **autómatas deterministas** como aquellos que están determinados unívocamente por la tabla de movimientos ya que hay **una y solo una** forma de que la máquina cambie su estado en una situación particular. Por contra, en el mismo texto se definen los **autómatas no deterministas** como los que 

# 4. Describir qué es un transductor secuencial. ¿Qué es la morfología de dos niveles?

Un **transductor de estados finitos** se puede ver cono un tipo de autómata finito que hace asociaciones entre dos conjuntos de símbolos. En particular, los **transductores secuenciales** son un tipo de transductores los cuales son determinísticos en su entrada. Cabe recalcar que los transdcutores secuenciales lo son en su entrada (*input*) pero no tienen porqué serlo en su salida (*output*).

**Relacionar transductor con morfología de dos niveles**

Una tarea imporante en el procesamiento del lenguaje natural es el **análisis morfológico** (*morphological parsing*, en inglés) o **morfología**. Según la Real Academia de la Lengua Española, la **morfología** es la parte de la gramática que **estudia la estructura de las palabras y de sus elementos constitutivos**. En el caso de la **morfología de dos niveles**, una palabra se representa como una correspondencia entre dos niveles: 

- **nivel léxico**: que representa la concatenación de los morfemas que componen una palabra.
- **nivel de superficie**: que representa la concatenación de las letras que componen la ortografía de una palabra.

El nivel de superficie podría verse como un nivel explícito, que viene dado por las letras que componen una palabra y el segundo nivel, el nivel léxico, es un nivel implícito y más abstracto. 



# 7. Describir la relación que existe entre los conceptos **corpus de entrenamiento** (*training set*), **corpus de test** (*test set*), **vocabulario abierto**, **vocabulario cerrado** y **perplejidad**.

Cualquier área del conocimiento que involucre el desarrollo de modelos matemáticos para conocer la realidad, se necesita entrenar sobre unos datos empíricos. En el área de la Inteligencia Artificial o del Machine Learning, hay varias estrategias para construir los modelos matemáticos a partir de unos datos. Una de ellas es dividir la muestra original en dos conjuntos: el conjunto de entrenamiento y el conjunto de test. En el procesamiento del lenguaje natural no es una excepción. Determinar la construcción del modelo, por ejemplo, un modelo de N-gramas, 

El **corpus de entrenamiento** es aquella subconjunto del corpus sobre el que se entrena el modelo, es decir, a partir de él se calculan los parámetros que rigen el modelo que en el contexto de los modelos de N-gramas pueden ser el N más adecuado, la mejor forma de *smoothing*, etcétera. No obstante, también necesitamos conocer cómo se corportará ese modelo en un entorno real. Es el papel que desempeña el **corpus de test**, otro subconjunto del corpus original distinto del corpus de entrenamiento. Si solo nos restringiésemos al corpus de entrenamiento tendríamos una información sesgada de cómo se desenvolvería en la realidad. Además, podríamos correr el riesgo de caer en el sobreajuste, es decir, tener un modelo excesivamente particularizado para el corpus de entrenamiento pero que, no generaliza bien fuera de él. 

De la mano de estos dos conceptos, tenemos el vocabulario abierto y el vocabulario cerrado. La hipótesis de vocabulario cerrado asume que **hay un léxico cerrado** y que todas las palabras que aparecen en el corpus de test están recogidas en este léxico.
Sin embargo, dada la riqueza del lenguaje, es ingenuo pensar que todas las palabras que puedan aparecer en el corpus de test están recogidas en ese léxico cerrado. Por lo tanto, necesitamos también el concepto de **vocabulario abierto** y en el que se trata de modelar las palabras desconocidas en el corpus de test.

Además del corpus de test, es necesario **definir alguna métrica** para conocer cómo de bien generaliza un modelo entrenado en el corpus de entrenamiento cuando se enfrenta al corpus de test. Una métrica es la **perplejidad**. 
La intuición de la métrica de perplejidad, según el libro base, se puede entender como *dados dos modelos probabilísticos, el mejor modelo será aquél que con un mejor ajuste al corpus de test o que mejor prediga los detalles del corpus de test*.



# 8. ¿Qué ocurre si tenemos un corpus de entrenamiento grande y un corpus de test pequeño y viceversa?


# 9. Describir qué problema se pretende resolver mediante las técnicas de *smooothing*.

Para cualquier modelo basado en **N-gramas** se necesitará una estimación de la máxima verosimilitud (EMV) obtenida del conjunto de entrenamiento. Estas estimaciones podrán ser precisas para un *N-grama* en el caso de que tenga un número suficiente de casos en el conjunto de entrenamiento. Sin embargo, el conjunto de datos en el que nos basamos para estimar estas probabilidades **es finito** y por tanto puede que secuencias de palabras perfectamente válidas no tengan ninguna ocurrencia lo que nos llevaría a tener **N-gramas con probabilidad 0** pero que no deberían. Además, nos enfrentamos al problema de que no solo podemos tener *N-gramas* con ninguna ocurrencia, sino que la estimación de máxima verosimilitud también será imprecisa en el caso en el que haya pocas ocurrencias de un determinado *N-grama*.

Por ejemplo, supongamos que nuestro corpus se compone de textos extraídos de libros de filosofía. N-gramas como *"teoría de la justicia"* o *"la libertad de pensamiento"* serán relativamente comunes y, por tanto, tendremos disponibles suficientes casos como para poder calcular la estimación de máxima verosimilitud. Sin embargo, seguramente no encontraremos ningún N-grama como *"el Real Madrid perdió"* y, por tanto, su estimación por máxima verosimilitud será 0. ¿Quiere decir esto que no podamos encontrar ese **4-grama**? Desde luego que no. Ese es precisamente el problema que se intenta subsanar, tratar de modificar la EMV centrándonos en aquellos *N-gramas* que hemos asumido incorrectamente que tendrían probabilidad 0. 


# 1. Describir el problema de la ambigüedad gramatical (*part-of-speech*)

Otra de las tareas fundamentales del procesamiento del lenguaje natural es la de conocer el papel que desempeñan cada una de las palabras que aparacen en una oración. Uno de los problemas a los que esta tarea puede enfrentarse es la ambigüedad gramatical, es decir, palabras con la misma grafía pueden desempeñar funciones distintas en una oración que dependerá de factores como las palabras que lo precedan e, incluso, el contexto en el que aparezca. 

La ambigüedad gramatical puede ser de distintos tipos según **cita**: 

- **ambigüedad estructural**: Se da en una oración o frase cuando tiene dos o más significados posibles, debido a la estructura, ya sea por el agrupamiento o la distinta función gramatical.

- **ambigüedad estructural**:

- **semántica**:

Una forma de ambigüedad gramatical es la ambigüedad gramatical en la que una misma palabra puede pertenecer a diversas categorías gramaticales. Un ejemplo clásico es la ambigüedad con la palabra *haya*, como puede verse en estas frases:

- He plantado un **haya** en mi jardín.
- Ojalá **haya** una solución al conflicto.

En la primera de ellas, la palabra *haya* se refiere al árbol y, por tanto, su categoría gramatical es la de **nombre**; en la segunda, *haya* se refiere a la tercera persona del singular del verbo haber. 

# 3. Complejidad computacional de ambigüedad gramatical. ¿Cómo lo resuelven los HMM?

Ya hemos visto que calcular 

$$
\hat{t}_1^n = \arg\max_{t_1^n}P(w_1^n|t_1^n)P(t_1^n)
$$
es **costoso computacionalmente**. Los modelos matemáticos intentan describir la realidad a través de ciertas hipótesis que permitan hacer una simplificación de la realidad y hacer posible los cálculos computacionales. Para el caso de la ambigüedad gramatical, tenemos los Modelos ocultos de Markov (HMM, por sus siglas en inglés) que simplifican la realidad asumiendo que:

1. La probabilidad de que aparezca una palabra depende solo de su función en la oración (*part-of-speech tag*). Esto se traduce, formalmente en

$$
P(w_1^n|t_1^n) \approx P(w_i|t_i)
$$
2. La probabilidad de que una etiqueta aparezca depende solo de la etiqueta anterior y no de la secuencia completa, es decir,
$$
P(t_1^n) \approx P(t_i|t_{i-1})
$$

Por lo tanto, la ecuación **bla** se convierte en 

$$
\hat{t}_1^n = \underset{t_1^n}{\arg\max} \, P(w_1^n|t_1^n)P(t_1^n)\approx \underset{t_1^n}{\arg\max} \,P(w_i|t_i) P(t_i|t_{i-1})
$$

Con un diagrama puede entenderse mejor en qué consisten los HMM:


# 4. ¿Cómo incluye la aproximación de Brill (Transformation-Based Tagging) las ventajas del modelo estocástico y de la aproximación basada en reglas?

La aproximación incluye tanto las ideas de los modelos estocásticos como de las aproximaciones basadas en reglas. ¿Por qué?

Por un lado, el algoritmo supone que hay una catálogo con las distinas estructuras de reglas que pueden existir que son:

- *Cambia la etiqueta **a** a la etiqueta **b** cuando:*
  - la palabra anterior (siguiente) está etiquetada como **z**.
  - dos palabras anterior (siguientes) están etiquedatas como **z**.
  - una de las dos palabras anteriores (siguientes) están etiquetadas como **z**.
  - Una de las tres palabras anteriores (siguientes) están etiquetads como **z**.
  - la palabra anterior está etiquetada como **z** y la palabra siguiente está etiquetada como **w**.
  - la palabra anterior (siguiente) está etiquedad como **z** y la dos palabras anteriores (siguientes) está etiquetadas como **w**.
  
Cada una de las variables **a**, **b**, **z** y **w** se refieren a las distintas funciones gramaticales.

Pero estas reglas se apoyan en los modelos estocásticos para terminar de completarse y se nutren de un conjunto de entrenamiento ya etiquetado para aprender cómo se han de construir, lo que suele denominarse **modelo supervisado** en el campo del **Aprendizaje Automático**.



# Bibliografía