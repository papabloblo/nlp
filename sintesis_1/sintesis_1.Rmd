---
title: "Síntesis 1"
output: html_notebook
bibliography: /Users/pablohidalgo/Documents/library.bib
---

# Introducción

# 2. Decribir la relación entre autómatas deterministas y no deterministas. El concepto de estrategia de búsqueda.

En la teoría de autómatas, una máquina de estados finitos se considera un autómata finito determinista si:

- cada una de sus transiciones está determinada unívocamente por su estado de entrada y su símbolo de entrada
- 

Estrictamente, un autómata determinista se puede considerar como un tipo de autómata no determinista

En [@Rabin1959] se definen los **autómatas deterministas** como aquellos que están determinados unívocamente por la tabla de movimientos ya que hay **una y solo una** forma de que la máquina cambie su estado en una situación particular. Por contra, en el mismo texto se definen los **autómatas no deterministas** como los que 

# 4. Describir qué es un transductor secuencial. ¿Qué es la morfología de dos niveles?

Un **transductor de estados finitos** se puede ver cono un tipo de autómata finito que hace asociaciones entre dos conjuntos de símbolos. En particular, los **transductores secuenciales** son un tipo de transductores los cuales son determinísticos en su entrada. Cabe recalcar que los transdcutores secuenciales lo son en su entrada (*input*) pero no tienen porqué serlo en su salida (*output*).

**Relacionar transductor con morfología de dos niveles**

Una tarea imporante en el procesamiento del lenguaje natural es el **análisis morfológico** (*morphological parsing*, en inglés) o **morfología**. Según la Real Academia de la Lengua Española, la **morfología** es la parte de la gramática que **estudia la estructura de las palabras y de sus elementos constitutivos**. En el caso de la **morfología de dos niveles**, una palabra se representa como una correspondencia entre dos niveles: 

- **nivel léxico**: que representa la concatenación de los morfemas que componen una palabra.
- **nivel de superficie**: que representa la concatenación de las letras que componen la ortografía de una palabra.

El nivel de superficie podría verse como un nivel explícito, que viene dado por las letras que componen una palabra y el segundo nivel, el nivel léxico, es un nivel implícito y más abstracto.



# 7. Describir la relación que existe entre los conceptos **corpus de entrenamiento** (*training set*), **corpus de test** (*test set*), **vocabulario abierto**, **vocabulario cerrado** y **perplejidad**.

Cualquier área del conocimiento que involucre el desarrollo de modelos matemáticos para conocer la realidad, se necesita entrenar sobre unos datos empíricos. En el área de la Inteligencia Artificial o del Machine Learning, hay varias estrategias para construir los modelos matemáticos a partir de unos datos. Una de ellas es dividir la muestra original en dos conjuntos: el conjunto de entrenamiento y el conjunto de test. En el procesamiento del lenguaje natural no es una excepción. Determinar la construcción del modelo, por ejemplo, un modelo de N-gramas, 

El **corpus de entrenamiento** es aquella subconjunto del corpus sobre el que se entrena el modelo, es decir, a partir de él se calculan los parámetros que rigen el modelo que en el contexto de los modelos de N-gramas pueden ser el N más adecuado, la mejor forma de *smoothing*, etcétera. No obstante, también necesitamos conocer cómo se corportará ese modelo en un entorno real. Es el papel que desempeña el **corpus de test**, otro subconjunto del corpus original distinto del corpus de entrenamiento. Si solo nos restringiésemos al corpus de entrenamiento tendríamos una información sesgada de cómo se desenvolvería en la realidad. Además, podríamos correr el riesgo de caer en el sobreajuste, es decir, tener un modelo excesivamente particularizado para el corpus de entrenamiento pero que, no generaliza bien fuera de él. 

De la mano de estos dos conceptos, tenemos el vocabulario abierto y el vocabulario cerrado. La hipótesis de vocabulario cerrado asume que **hay un léxico cerrado** y que todas las palabras que aparecen en el corpus de test están recogidas en este léxico.
Sin embargo, dada la riqueza del lenguaje, es ingenuo pensar que todas las palabras que puedan aparecer en el corpus de test están recogidas en ese léxico cerrado. Por lo tanto, necesitamos también el concepto de **vocabulario abierto** y en el que se trata de modelar las palabras desconocidas en el corpus de test.

Además del corpus de test, es necesario **definir alguna métrica** para conocer cómo de bien generaliza un modelo entrenado en el corpus de entrenamiento cuando se enfrenta al corpus de test. Una métrica es la **perplejidad**. 
La intuición de la métrica de perplejidad, según el libro base, se puede entender como *dados dos modelos probabilísticos, el mejor modelo será aquél que con un mejor ajuste al corpus de test o que mejor prediga los detalles del corpus de test*.



# 8. ¿Qué ocurre si tenemos un corpus de entrenamiento grande y un corpus de test pequeño y viceversa?


# 9. Describir qué problema se pretende resolver mediante las técnicas de *smooothing*.

Para cualquier modelo basado en **N-gramas** se necesitará una estimación de la máxima verosimilitud (EMV) obtenida del conjunto de entrenamiento. Estas estimaciones podrán ser precisas para un *N-grama* en el caso de que tenga un número suficiente de casos en el conjunto de entrenamiento. Sin embargo, el conjunto de datos en el que nos basamos para estimar estas probabilidades **es finito** y por tanto puede que secuencias de palabras perfectamente válidas no tengan ninguna ocurrencia lo que nos llevaría a tener **N-gramas con probabilidad 0** pero que no deberían. Además, nos enfrentamos al problema de que no solo podemos tener *N-gramas* con ninguna ocurrencia, sino que la estimación de máxima verosimilitud también será imprecisa en el caso en el que haya pocas ocurrencias de un determinado *N-grama*.

Por ejemplo, supongamos que nuestro corpus se compone de textos extraídos de libros de filosofía. N-gramas como *"teoría de la justicia"* o *"la libertad de pensamiento"* serán relativamente comunes y, por tanto, tendremos disponibles suficientes casos como para poder calcular la estimación de máxima verosimilitud. Sin embargo, seguramente no encontraremos ningún N-grama como *"el Real Madrid perdió"* y, por tanto, su estimación por máxima verosimilitud será 0. ¿Quiere decir esto que no podamos encontrar ese **4-grama**? Desde luego que no. Ese es precisamente el problema que se intenta subsanar, tratar de modificar la EMV centrándonos en aquellos *N-gramas* que hemos asumido incorrectamente que tendrían probabilidad 0. 

# Bibliografía