---
title: "Síntesis 1"
output:
  html_notebook:
    number_sections: yes
    toc: yes
  pdf_document:
    toc: yes
bibliography: /Users/pablohidalgo/Documents/library.bib
---

# Introducción

# 1. Describir la relación existente entre las expresiones regulares y los autómatas finitos. A modo de ejemplo, ilustrarlo con alguna de la relacionadas con la práctica y su autómata correspondiente.

El procesamiento del lenguaje natural aspira a hacer 

En el procesamiento del lenguaje natural trabajamos con palabaras o, lo que es lo mismo, con **cadenas de caracteres**. 

Una **expresión regular** es una notación algebraica que permite caracterizar un conjunto de cadenas de caracteres. Dicho de otro modo, mediante una concatenación de símbolos, seremos capaces de encontrar cadenas de caracteres que sigan un patrón o estructura determinada. Podría denominarse como un *metalenguaje*. 

Pero estas notaciones algebraicas no tienen ninguna utilidad en sí mismas, necesitamos que un ordenador sea capaz de entenderlas para llevarlas a la práctica. Es aquí donde entran en juego los **autómatas finitos**, que pueden verse como una implementación de las expresiones regulares. De hecho, cualquier expresión regular puede ser implementada como un autómata finito y viceversa, cualquier autómata finito puede ser descrito con una expresión regular.

**EJEMPLO DE LA PRÁCTICA**.

# 2. Decribir la relación entre autómatas deterministas y no deterministas. El concepto de estrategia de búsqueda.

Podemos distinguir dos tipos de autómatas finitos: determinísticos y no determinísticos. Recordemos que se pueden representar los autómatas finitos con la tabla movimientos o de estado-transición. En esta tabla se recogen el estado inicial, los estados aceptados y qué transiciones se permiten entre estados en función de los símbolos. 

En [@Rabin1959] se definen los **autómatas deterministas** como aquellos que están determinados unívocamente por la tabla de estado-transición ya que hay **una y solo una** forma de que el autómata cambie su estado en una situación particular. Por contra, en el mismo texto se definen los **autómatas no deterministas** como aquellos que tienen estados cuyos movimientos no están unívocamente determinados, es decir, llegado a un estado el símbolo para ir al siguiente estado no determina estrictamente el siguiente estado. 

Estrictamente, un autómata determinista se puede considerar como un tipo de autómata no determinista. 

En un primer acercamiento podría parecer que los autómatas no deterministas son más potentes que los deterministas. Sin embargo, en [@Rabin1959] se prueba que **cualquier autómata no determinista se puede convertir en uno determinista**. Este resultado es interesante ya que podemos construir un autómata no determinista de forma fácil y que se convierta en uno determinista que tiene la ventaja de ser más eficiente computacionalmente. No obstante, en la práctica esta construcción puede ser inviable ya que el autómata determinista equivalente a uno no determinista con $N$ estados puede llegar a tener $2^N$ estados.

Como hemos visto, los autómatas no deterministas tienen puntos de elección en los que existen varias posibilidades a seguir. Los algoritmos de reconocimiento para autómatas no deterministas deberán elegir una **estrategia de búsqueda** para tratar estos puntos de elección. Fundamentalmente hay dos estrategias: 

- **primero en profundidad o LIFO** (*last in first out*, último en entrar, primero en salir): dado un punto de elección, se enumeran todos los siguientes movimientos posibles, se escoge el último que se ha recogido, y se sigue hasta que se obtiene la aceptación rechazo de la cadena de caracteres. Si se rechaza, se sigue con el sigueinte estado posible que no haya sido considerado. 

- **primero en anchura o FIFO** (*first in first out*, primero en entrar, primero en salir): en vez de escoger un camino como en la estrategia LIFO, se examinan todas las posibles elecciones pero aumentando progresivamente un paso cada una de las elecciones.

**¿DIBUJO?**

Para problemas complejos, se pueden utilizar otras estrategias de búsqueda como la **programación dinámica** o **A\***.


En la teoría de autómatas, una máquina de estados finitos se considera un autómata finito determinista si:

- cada una de sus transiciones está determinada unívocamente por su estado de entrada y su símbolo de entrada
- 

# 3.Describir para el inglés y el castellano los elementos de la morfología: ejemplos de morfemas, prefijos, sufijos, infijos, circunfijos, derivación, inflexión, concordancia y enclíticos. Utilizar los recursos recomendados (lematizador de la U. de las Palmas y MACO)

Una tarea imporante en el procesamiento del lenguaje natural es el **análisis morfológico** (*morphological parsing*, en inglés) o **morfología**. Según la **Real Academia de la lengua Española (RAE)**, la **morfología** es la parte de la gramática que **estudia la estructura de las palabras y de sus elementos constitutivos**. Las **palabras** son unidades de la lengua dotadas de significado y están constituidas por una o varias unidades menores, también con significado, llamados morfemas.

En clastellano existen dos tipos distintos de morfemas: los **morfemas léxicos o lexemas** y los **morfemas** que se añaden a ese lexema. En inglés se denominan *stems* y *affixes*, respectivamente.

El **morfema léxico o lexema** es el morfema que aporta significado básico a la palabara. Por ejemplo: en *carpintero*, la raíz es *carpint-*. 

Los **morfemas yqe se añaden a la raíz** aportan significados que se agregan al principal modificándolo en parte. Se pueden distinguir dos tipos:

- los **morfemas derivativos** o **afijos**, que se añaden a la ráiz para formar otra palabra distinta.

- los **morfemas flexivos** o **desinencias**, que dan lugar a distintas formas de una misma palabra. Indican género, persona, número, etc.

Existen diversos tipos de procedimientos de formación de palabras. Uno de ellos es la **derivación** que consiste en añadir un afijo a la raíz de una palabra para formar otra nueva. Los **afijos** están categorizados en:

- **prefijos**, aquellos que se anteponen a una palabra. Por ejemplo, poner y **re**poner, moral y **a**moral, hacer y **des**hacer.
- **sufijos**, aquellos que se añaden detrás de la raíz o lexema. Por ejemplo, rico y riqu**eza**, fenómeno y fenomne**al**. Al añadie un sufijo a una raíz se producen cambios en el significado y, con frecuencia, en la categoría de la palabra original.
- **interfijos**, aquellos que se añaden entre la base y un sufijo derivativo o entre la base y la flexión verbal. Por ejemplo, picar y pic**aj**oso, apretar y apret**uj**ar.

Existen otro tipos de afijos llamados **circunfijos** constituido por dos segementos discontinuos, uno que precede y otro que sigue al lexema o base al que se junta [@Alcaraz1997]. Se puede considerar una combinación de prefijo y sufijo pero con contenido gramatical único. Por ejemplo, la palabra indonesia *kebisaan* (capacidad)
 está formada por una base de carácter verbal, *bisa* (ser capaz) y un circunfijo, *ke-...-an* [@Alcaraz1997].

Otro mecanismo para la creación de palabras es la **composición** que consiste en en unir dos o más raíces o palabras. Por ejemplo saca + corchos = **saca***corchos*, agrio + dulce = **agri***dulce*.

La **inflexión**  es el proceso morfológico consistente en la modificación de palabras mediante morfemas flexivos, es decir, afijos con significados o valores gramaticales tales como *género*, *número*, *tiempo* y *persona*. A diferencia de la *derivación*, la inflexión está generalmente determinada por las relaciones sintácticas que se establecen entre las palabras. Por ejemplo, casa y casa**s*.



# 4. Describir qué es un transductor secuencial. ¿Qué es la morfología de dos niveles?

Un **transductor de estados finitos** se puede ver cono un tipo de autómata finito que hace asociaciones entre dos conjuntos de símbolos. En particular, los **transductores secuenciales** son un tipo de transductores los cuales son determinísticos en su entrada. Cabe recalcar que los transdcutores secuenciales lo son en su entrada (*input*) pero no tienen porqué serlo en su salida (*output*).

**Relacionar transductor con morfología de dos niveles**

Una tarea imporante en el procesamiento del lenguaje natural es el **análisis morfológico** (*morphological parsing*, en inglés) o **morfología**. Según la Real Academia de la Lengua Española, la **morfología** es la parte de la gramática que **estudia la estructura de las palabras y de sus elementos constitutivos**. En el caso de la **morfología de dos niveles**, una palabra se representa como una correspondencia entre dos niveles: 

- **nivel léxico**: que representa la concatenación de los morfemas que componen una palabra.
- **nivel de superficie**: que representa la concatenación de las letras que componen la ortografía de una palabra.

El nivel de superficie podría verse como un nivel explícito, que viene dado por las letras que componen una palabra y el segundo nivel, el nivel léxico, es un nivel implícito y más abstracto. 

# 5. Describir en qué consiste la intersección, proyección y composición de transductores y qué fenómenos lingüísticos pueden tratarse con esta aproximación.

# 7. Describir la relación que existe entre los conceptos **corpus de entrenamiento** (*training set*), **corpus de test** (*test set*), **vocabulario abierto**, **vocabulario cerrado** y **perplejidad**.

Cualquier área del conocimiento que involucre el desarrollo de modelos matemáticos para conocer la realidad, se necesita entrenar sobre unos datos empíricos. En el área de la Inteligencia Artificial o del Machine Learning, hay varias estrategias para construir los modelos matemáticos a partir de unos datos. Una de ellas es dividir la muestra original en dos conjuntos: el conjunto de entrenamiento y el conjunto de test. En el procesamiento del lenguaje natural no es una excepción. Determinar la construcción del modelo, por ejemplo, un modelo de N-gramas, 

El **corpus de entrenamiento** es aquella subconjunto del corpus sobre el que se entrena el modelo, es decir, a partir de él se calculan los parámetros que rigen el modelo que en el contexto de los modelos de N-gramas pueden ser el N más adecuado, la mejor forma de *smoothing*, etcétera. No obstante, también necesitamos conocer cómo se corportará ese modelo en un entorno real. Es el papel que desempeña el **corpus de test**, otro subconjunto del corpus original distinto del corpus de entrenamiento. Si solo nos restringiésemos al corpus de entrenamiento tendríamos una información sesgada de cómo se desenvolvería en la realidad. Además, podríamos correr el riesgo de caer en el sobreajuste, es decir, tener un modelo excesivamente particularizado para el corpus de entrenamiento pero que, no generaliza bien fuera de él. 

De la mano de estos dos conceptos, tenemos el vocabulario abierto y el vocabulario cerrado. La hipótesis de vocabulario cerrado asume que **hay un léxico cerrado** y que todas las palabras que aparecen en el corpus de test están recogidas en este léxico.
Sin embargo, dada la riqueza del lenguaje, es ingenuo pensar que todas las palabras que puedan aparecer en el corpus de test están recogidas en ese léxico cerrado. Por lo tanto, necesitamos también el concepto de **vocabulario abierto** y en el que se trata de modelar las palabras desconocidas en el corpus de test.

Además del corpus de test, es necesario **definir alguna métrica** para conocer cómo de bien generaliza un modelo entrenado en el corpus de entrenamiento cuando se enfrenta al corpus de test. Una métrica es la **perplejidad**. 
La intuición de la métrica de perplejidad, según el libro base, se puede entender como *dados dos modelos probabilísticos, el mejor modelo será aquél que con un mejor ajuste al corpus de test o que mejor prediga los detalles del corpus de test*.



# 8. ¿Qué ocurre si tenemos un corpus de entrenamiento grande y un corpus de test pequeño y viceversa?


# 9. Describir qué problema se pretende resolver mediante las técnicas de *smooothing*.

Para cualquier modelo basado en **N-gramas** se necesitará una estimación de la máxima verosimilitud (EMV) obtenida del conjunto de entrenamiento. Estas estimaciones podrán ser precisas para un *N-grama* en el caso de que tenga un número suficiente de casos en el conjunto de entrenamiento. Sin embargo, el conjunto de datos en el que nos basamos para estimar estas probabilidades **es finito** y por tanto puede que secuencias de palabras perfectamente válidas no tengan ninguna ocurrencia lo que nos llevaría a tener **N-gramas con probabilidad 0** pero que no deberían. Además, nos enfrentamos al problema de que no solo podemos tener *N-gramas* con ninguna ocurrencia, sino que la estimación de máxima verosimilitud también será imprecisa en el caso en el que haya pocas ocurrencias de un determinado *N-grama*.

Por ejemplo, supongamos que nuestro corpus se compone de textos extraídos de libros de filosofía. N-gramas como *"teoría de la justicia"* o *"la libertad de pensamiento"* serán relativamente comunes y, por tanto, tendremos disponibles suficientes casos como para poder calcular la estimación de máxima verosimilitud. Sin embargo, seguramente no encontraremos ningún N-grama como *"el Real Madrid perdió"* y, por tanto, su estimación por máxima verosimilitud será 0. ¿Quiere decir esto que no podamos encontrar ese **4-grama**? Desde luego que no. Ese es precisamente el problema que se intenta subsanar, tratar de modificar la EMV centrándonos en aquellos *N-gramas* que hemos asumido incorrectamente que tendrían probabilidad 0. 

# Etiquetado

Una de las tareas del procesamiento del lenguaje natural es la de saber a qué **categoría gramatical** pertenece cada una de las palabras que están presentes en una frase o texto (*Part-of-Speech Tagging* en inglés). Esta tarea nos ayudará a desarrollar con éxito otras como la del reconocimiento del habla. 

No se trata de una tarea sencilla ya que la riqueza del lenguaje hace que haya que enfrentarse a palabras con la misma grafía pero que pueden pertenecer a categorías gramaticales distintas en una oración lo que se conoce como **ambigüedad gramatical**.

En caso de ambigüedad, la categoría gramatical puede depender de factores como las palabras precedentes o el contexto en el que se desarrolla la oración. La palabra **haya** es un ejemplo de ambigüedad gramatical:

- He plantado un **haya** en mi jardín.
- Ojalá **haya** una solución al conflicto.

En el primer ejemplo, la palabra *haya* se refiere al árbol y, por tanto, su categoría gramatical es la de **nombre**; en la segunda, *haya* se refiere a la tercera persona del singular del verbo haber. En castellano, una primera categorización gramatical aparece en la Gramática Castellana de Antonio de Nebrija en 1492 (falta cita). Posteriormente, los lingüistas han enriquecido esta gramática con una recategorización.

En el contexto del procesamiento del lenguaje natural, como rama de la Inteligencia  Artificial, se han desarrollado distintos algoritmos que permitan realizar el etiquetado gramatical de forma automática, sin la intervención humana ya que el objetivo es la de poder etiquetar corpus de gran tamaño y que resultaría una tarea inabarcable para un humano.

Se pueden distinguir tres grandes enfoques para el etiquetado gramatical: los etiquetadores **basados en reglas**, los **estocásticos** y aquellos que combinan los dos anteriores.

## Etiquetadores basados en reglas. EngCG ENGTWOL.

Los etiquetadores basados en reglas tratan de conocer la categoría gramatical aplicando estructuras lógicas preconcebidas. 

El etiquetador EngCG ENGTWOL es uno de estos etiquetadores basados en reglas. La información que nutre a este etiquetador gramatical son los **diccionarios** y los **conjuntos de reglas**.

El etiquetador EngCG ENGTWOL contiene un **diccionario con más de 56.000 entradas** en el que se recoge la función gramatical que puede desempeñar dada la raíz o morfema (*stem*) de una palabra en inglés. En el caso de que una misma raíz pueda puede desempeñar varias funciones gramaticales, existirá una entrada separada en el diccionario para cada una de ellas con un conjunto de caractarerísticas tanto morfológicas como sintácticas. En el diccionario no aparecen formas derivadas o ¿*inflected*?.

Por otro lado, también tiene un **conjunto de reglas** que son utilizadas para subsanar la posible ambigüedad gramatical. Estas reglas son utilizadas en negativo, es decir, cada una de ellas lleva asociada una serie de condiciones que según se satisfagan o no, supondrán la eliminación de una de las posibles ambigüedades. El conjunto de reglas está constituido por 3.744 en el sistema EngCG-2. **¡COMPLETAR!**

## Etiquetadores estocásticos. HMM.

Los etiquetadores basados en reglas tienen el inconveniente de que las reglas han de ser determinadas por un grupo de expertos. El enfoque de los etiquetadores estocásticos es el de hacer que un algoritmo sea capaz de determinar el etiquetado aprendiendo de un corpus ya etiquetado e inferir las etiquetas para corpus de test. Tiene también la contrapartida de necesitar un corpus etiquetado con el esfuerzo que ello pede suponer. 

Desde el punto de vista estocástico, los etiquetadores tratan de encontrar aquella sucesión de etiquetas gramaticales $t_1^n = (t_1, \ldots, t_n)$ que se considere la más probable para una secuencia de palabras dada, es decir, estamos tratando de resolver el problema

$$
\hat{t}_1^n = \underset{t_1^n}{\arg\max}\, P(t_1^n|w_1^n)
$$

Aplicando el teorema de Bayes, llegamos a 

$$
\hat{t}_1^n = \arg\max_{t_1^n}P(w_1^n|t_1^n)P(t_1^n)
$$

Los modelos ocultos de Markov (*HMM* por sus siglas en inglés) pertenecen a la categoría de los etiquetadores estocásticos.

Ya hemos visto que calcular 


es **costoso computacionalmente**. Los modelos matemáticos intentan describir la realidad a través de ciertas hipótesis que permitan hacer una simplificación de la realidad y hacer posible los cálculos computacionales. Para el caso del etiquetado gramatical, tenemos los Modelos ocultos de Markov (HMM, por sus siglas en inglés) que simplifican la realidad asumiendo que:

1. La probabilidad de que aparezca una palabra depende solo de su función en la oración (*part-of-speech tag*), es decir,

$$
P(w_1^n|t_1^n) \approx P(w_i|t_i)
$$

2. La probabilidad de que una etiqueta aparezca depende solo de la etiqueta anterior y no de la secuencia completa, es decir,
$$
P(t_1^n) \approx P(t_i|t_{i-1})
$$

Por lo tanto, el problema a resolver se convierte en 

$$
\hat{t}_1^n = \underset{t_1^n}{\arg\max} \, P(w_1^n|t_1^n)P(t_1^n)\approx \underset{t_1^n}{\arg\max} \,P(w_i|t_i) P(t_i|t_{i-1})
$$

Que es más manejable computacionalmente. Uno de los algoritmos más comunes para calcular la secuencia de etiquetas $t_1^n$ más probables es el **Algoritmo de Viterbi**.

## Etiquetadores híbridos. Aproximación de Brill.

Existen algoritmos híbridos que heredan las ideas tanto de los etiquetadores estocásticos como de los de basados en reglas. Conjugan un conjunto de reglas formales estructuradas con el perfeccionamiento de estas reglas mediante el entrenamiento en un corpus de entrenamiento.

En primer lugar, el algoritmo etiqueta cada palabra con su categoría más probable, es decir, $\hat{t}_i = \underset{t_i}{argmax}P(t_i|w_i)$. Después, aplica el conjunto de reglas aprendidas para modificar algunas de las etiquetas.

El algoritmo supone un catálogo con las distinas estructuras de reglas que pueden existir:

- *Cambia la etiqueta **a** a la etiqueta **b** cuando:*
  - la palabra anterior (siguiente) está etiquetada como **z**.
  - dos palabras anterior (siguientes) están etiquedatas como **z**.
  - una de las dos palabras anteriores (siguientes) están etiquetadas como **z**.
  - Una de las tres palabras anteriores (siguientes) están etiquetads como **z**.
  - la palabra anterior está etiquetada como **z** y la palabra siguiente está etiquetada como **w**.
  - la palabra anterior (siguiente) está etiquedad como **z** y la dos palabras anteriores (siguientes) está etiquetadas como **w**.
  
Cada una de las variables **a**, **b**, **z** y **w** se refieren a las distintas funciones gramaticales.

Estas reglas se apoyan en los modelos estocásticos para terminar de completarse y se nutren de un corpus de entrenamiento ya etiquetado para aprender cómo se han de construir, lo que suele denominarse **modelo supervisado** en el campo del **Aprendizaje Automático**.

# 1. Describir el problema de la ambigüedad gramatical (*part-of-speech*)

Otra de las tareas fundamentales del procesamiento del lenguaje natural es la de conocer el papel que desempeñan cada una de las palabras que aparacen en una oración. Uno de los problemas a los que esta tarea puede enfrentarse es la ambigüedad gramatical, es decir, palabras con la misma grafía pueden desempeñar funciones distintas en una oración que dependerá de factores como las palabras que lo precedan e, incluso, el contexto en el que aparezca. 

La ambigüedad gramatical puede ser de distintos tipos según **cita**: 

- **ambigüedad estructural**: Se da en una oración o frase cuando tiene dos o más significados posibles, debido a la estructura, ya sea por el agrupamiento o la distinta función gramatical.

- **ambigüedad estructural**:

- **semántica**:

Una forma de ambigüedad gramatical es la ambigüedad gramatical en la que una misma palabra puede pertenecer a diversas categorías gramaticales. Un ejemplo clásico es la ambigüedad con la palabra *haya*, como puede verse en estas frases:

- He plantado un **haya** en mi jardín.
- Ojalá **haya** una solución al conflicto.

En la primera de ellas, la palabra *haya* se refiere al árbol y, por tanto, su categoría gramatical es la de **nombre**; en la segunda, *haya* se refiere a la tercera persona del singular del verbo haber. 

# 2. ¿Qué información emplea el etiquedator gramatical EngCG ENGTWOL?

El etiquetador gramatical **EngCG ENGTWOL** cae dentro de aquellos **etiquetadores gramaticales basados en reglas**. Los dos componentes de este etiquetador son los **diccionarios** y los **conjuntos de reglas**.

El EngCG ENGTWOL tiene un diccionario con más de 56.000 entradas en el que se recoge la función gramatical que puede desempeñar dada la raíz (*stem*) de una palabra en inglés. En el caso de que una misma raíz pueda puede desempeñar varias funciones gramaticales, existirá una entrada separada en el diccionario para cada una de ellas. En el diccionario no aparecen formas derivadas o ¿*inflected*?. En cada una de las entradas se tiene un conjunto de características morfológicas y sintácticas.

Por otro lado, también tiene un conjunto de reglas que son utilizadas para subsanar la posible ambigüedad gramatical. Estas reglas son utilizadas en negativo, es decir, cada una de ellas lleva asociada una serie de condiciones que según se satisfagan o no, supondrán la eliminación de una de las posibles ambigüedades. El conjunto de reglas está constituido por 3.744 en el sistema EngCG-2

# 3. Complejidad computacional de ambigüedad gramatical. ¿Cómo lo resuelven los HMM?

Ya hemos visto que calcular 

$$
\hat{t}_1^n = \arg\max_{t_1^n}P(w_1^n|t_1^n)P(t_1^n)
$$
es **costoso computacionalmente**. Los modelos matemáticos intentan describir la realidad a través de ciertas hipótesis que permitan hacer una simplificación de la realidad y hacer posible los cálculos computacionales. Para el caso de la ambigüedad gramatical, tenemos los Modelos ocultos de Markov (HMM, por sus siglas en inglés) que simplifican la realidad asumiendo que:

1. La probabilidad de que aparezca una palabra depende solo de su función en la oración (*part-of-speech tag*). Esto se traduce, formalmente en

$$
P(w_1^n|t_1^n) \approx P(w_i|t_i)
$$
2. La probabilidad de que una etiqueta aparezca depende solo de la etiqueta anterior y no de la secuencia completa, es decir,
$$
P(t_1^n) \approx P(t_i|t_{i-1})
$$

Por lo tanto, la ecuación **bla** se convierte en 

$$
\hat{t}_1^n = \underset{t_1^n}{\arg\max} \, P(w_1^n|t_1^n)P(t_1^n)\approx \underset{t_1^n}{\arg\max} \,P(w_i|t_i) P(t_i|t_{i-1})
$$

Con un diagrama puede entenderse mejor en qué consisten los HMM:


# 4. ¿Cómo incluye la aproximación de Brill (Transformation-Based Tagging) las ventajas del modelo estocástico y de la aproximación basada en reglas?

La aproximación incluye tanto las ideas de los modelos estocásticos como de las aproximaciones basadas en reglas. ¿Por qué?

Por un lado, el algoritmo supone que hay una catálogo con las distinas estructuras de reglas que pueden existir que son:

- *Cambia la etiqueta **a** a la etiqueta **b** cuando:*
  - la palabra anterior (siguiente) está etiquetada como **z**.
  - dos palabras anterior (siguientes) están etiquedatas como **z**.
  - una de las dos palabras anteriores (siguientes) están etiquetadas como **z**.
  - Una de las tres palabras anteriores (siguientes) están etiquetads como **z**.
  - la palabra anterior está etiquetada como **z** y la palabra siguiente está etiquetada como **w**.
  - la palabra anterior (siguiente) está etiquedad como **z** y la dos palabras anteriores (siguientes) está etiquetadas como **w**.
  
Cada una de las variables **a**, **b**, **z** y **w** se refieren a las distintas funciones gramaticales.

Pero estas reglas se apoyan en los modelos estocásticos para terminar de completarse y se nutren de un conjunto de entrenamiento ya etiquetado para aprender cómo se han de construir, lo que suele denominarse **modelo supervisado** en el campo del **Aprendizaje Automático**.



# Bibliografía